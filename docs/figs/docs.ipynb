{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuickML Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing VMWare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VMWare, or an equivalent (VirtualBox, etc.) needs to be installed to be able to run virtual envrionments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating a machine learning model is preparing the data to be fed into it by pre-processing. The data needs to be pre-processed and the following steps followed:\n",
    "\n",
    "1. Acquire the Dataset \n",
    "2. Import Necessary Libraries \n",
    "3. Import the Dataset\n",
    "4. Handling Missing Values\n",
    "5. Encoding Categorical Data\n",
    "6. Splitting into Training and Test Set\n",
    "7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing All Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping independent, dependent, categorical and missing data\n",
    "# to begin data pre-processing.\n",
    "var_map = {\n",
    "    \"independent\" : [\"R&D Spend\", \"Administration\",\"Marketing Spend\", \"State\"],\n",
    "    \"dependent\" : [\"Profit\"],\n",
    "    \"categorical\" : [\"State\"],\n",
    "    \"missing\": [\"Marketing Spend\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Function \n",
    "def dataPreProcess(dataSet, varMap):\n",
    "    # Obtaining Data Set\n",
    "    data_root = pd.read_csv(dataSet)\n",
    "    data = data_root.copy()\n",
    "\n",
    "    # Splitting Dependent & Independent Variables\n",
    "    X = data[varMap['independent']]  \n",
    "    y = data[varMap['dependent']]\n",
    "\n",
    "    # Removing any missing data\n",
    "    imputer = SimpleImputer(missing_values=np.nan , strategy='mean')\n",
    "    imputer = imputer.fit(X[varMap['missing']])\n",
    "    X[varMap['missing']] =imputer.transform(X[varMap['missing']])\n",
    "\n",
    "    # Encoding Categorical Variables\n",
    "    le = LabelEncoder()\n",
    "    X[varMap['categorical']]= pd.DataFrame(le.fit_transform(X[varMap['categorical']]))\n",
    "    col_tans = make_column_transformer( \n",
    "                         (OneHotEncoder(), \n",
    "                         varMap['categorical']))\n",
    "    Xtemp2 = col_tans.fit_transform(X[varMap['categorical']])\n",
    "    # Splitting Into Train and Test Set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3 , random_state = 0)\n",
    "\n",
    "    # Feature Scaling\n",
    "    scale_X = StandardScaler()\n",
    "    X_train.iloc[: , :] = scale_X.fit_transform(X_train.iloc[: , :])\n",
    "    X_test.iloc[: , :] = scale_X.fit_transform(X_test.iloc[: , :])\n",
    "\n",
    "    # Returns a dictionary of pre-processed data\n",
    "    return(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_train\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data processing function is responsible for taking a dataset and a mapping of dependent, independent, missing and categorical data. The dataset is split into the dependent and independent data, the missing data is taken care of, and the categorical data is encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the data is split into the test and train and it is feature scaled. The function returns a dictionary of the train and test matrices and vectors ready for a machine learning model to be fitted on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Dynamic Table Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Algorithm of choice is selected, an HTML table is dynamically created with the column names:\n",
    "1. Independent\n",
    "2. Dependent\n",
    "3. Categorical \n",
    "As well as dynamically created row names which correspond to the attributes in the inputted data set. This was done using Flask and Jinja. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the file the user submits is saved to a specific folder, effectively keeping a reference to this file to be used later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoked when user submits file - \n",
    "# creates HTML table with attributes of file\n",
    "@views.route('/', methods=['POST'])\n",
    "def upload_file():\n",
    "    \n",
    "    global filename\n",
    "    file = request.files['file']\n",
    "    \n",
    "    # Saves the file so it can be accessed later on.\n",
    "    dataSet = pd.read_csv(file)\n",
    "    file.save(os.path.join(UPLOAD_FOLDER, file.filename))\n",
    "    filename = file.filename\n",
    "\n",
    "    return render_template('home.html', attributes = list(dataSet.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    " {% for i in attributes: %}\n",
    "                <tr>\n",
    "                    <th class=\"rt\" value='{{i}}'>{{i}}</th>\n",
    "                    <td><input class='rd' type=\"radio\" name='test_{{attributes.index(i)}}' \n",
    "                               value=\"Ind\"></td>\n",
    "                    <td><input class='rd dep' type=\"radio\" name='test_{{attributes.index(i)}}'\n",
    "                               value=\"Dep\"></td>\n",
    "                    <td><input class='rd' type=\"checkbox\" value=\"Cat\"></td>\n",
    "                </tr>\n",
    "{% endfor %}\n",
    "<!-- Using Jinja python expressions can be written in html. \n",
    "    Table created dynamically using for loop. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dynamic Creation of Variable Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the radio buttons and checkboxes on the dynamically created table are used to create the mapping of attributes. Namely, the user selects which attributes are dependent, independent, and which are categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "function makeVarMap() {\n",
    "\n",
    "  const radioButtons = document.querySelectorAll('.rd');  // All radio buttons\n",
    "  const headers = document.querySelectorAll('th.rt');     // Headers\n",
    "  // Hard coded keys as they never change regardless of use case. \n",
    "  var varMap = {\n",
    "    'Independent': [],\n",
    "    'Dependent': [],\n",
    "    'Categorical': []\n",
    "  }\n",
    "\n",
    "  var head = [];\n",
    "  for (let i = 0; i < headers.length; ++i) {\n",
    "    head[i] = headers[i].textContent;\n",
    "  }\n",
    "  var j = 0;\n",
    "  // Loops through radio buttons \n",
    "  for (let x = 0; x < radioButtons.length; x++) {\n",
    "    if (radioButtons[x].checked && radioButtons[x].value == 'Ind') {\n",
    "      varMap['Independent'].push(head[j]);\n",
    "      j++;\n",
    "    }\n",
    "    if (radioButtons[x].checked && radioButtons[x].value == 'Dep') {\n",
    "      varMap['Dependent'].push(head[j]);\n",
    "      j++;\n",
    "    }\n",
    "    if (radioButtons[x].checked && radioButtons[x].value == 'Cat') {\n",
    "      // Decrements becase a categorical variable is always ALSO ind or dep.   \n",
    "      j--;\n",
    "      varMap['Categorical'].push(head[j]);\n",
    "      j++;\n",
    "      // Increments so order is not messed up.\n",
    "    }\n",
    "  }\n",
    "  console.log(varMap);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also some checkbox logic implemented such that a variable cannot be both independent and dependent and that there can only ever be one dependent variable in any inputted dataset. This was done in jQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "$(document).ready(function () {\n",
    "  $('input.dep:radio').change(function() {\n",
    "      // When any radio button on the page is selected,\n",
    "      // then deselect all other radio buttons.\n",
    "      $('input.dep:radio:checked').not(this).prop('checked', false);\n",
    "  });\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Passing Variable Mapping to be Pre Processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable mapping is created in JavaScript dnyamically using the users input. It is then passed to the python backend using AJAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "$.ajax({\n",
    "    url: '/dataPreProcessing',\n",
    "    type: \"POST\",\n",
    "    contentType: \"application/json\", \n",
    "    data: JSON.stringify(s)\n",
    "  }).done(function(result){     // on success get the return object from server\n",
    "    console.log(result)     // see it in the console to test its working \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Pre Processing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the variable mapping is created in the JavaScript, it is passed into the flask backend which takes the original file the user submitted, as well as the newly created variable mapping, passing both of them as arguments to the data pre-processing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoked when user submits variable mapping \n",
    "@views.route('/dataPreProcessing', methods=['POST'])\n",
    "def dataPre():\n",
    "    # result is the variable mapping in a JSON format\n",
    "    result =  request.get_json()\n",
    "\n",
    "    # Dataset and variable mapping to be passed into the data\n",
    "    # pre-processing function\n",
    "    varMap = json.loads(result)\n",
    "    file = os.path.join(UPLOAD_FOLDER, filename)\n",
    "\n",
    "    table = DPP.dataPreProcess(file, varMap)\n",
    "\n",
    "    # Getting the individual components of pre processed data \n",
    "    # to keep a reference to them for when they need to be passed \n",
    "    # in to the selected algorithm.\n",
    "    xTest = pd.DataFrame(table['X_test'])\n",
    "    xTrain = pd.DataFrame(table['X_train'])\n",
    "    yTest = pd.DataFrame(table['y_test'])\n",
    "    yTrain = pd.DataFrame(table['y_train'])   \n",
    "\n",
    "    # Creating variables to store file names and locations for pre \n",
    "    # processed data locations\n",
    "    fN_xT = '/home/user/Documents/git/QuickML/pre_processed_data/xTest'\n",
    "    fN_xTr = '/home/user/Documents/git/QuickML/pre_processed_data/xTrain'\n",
    "    fN_yT = '/home/user/Documents/git/QuickML/pre_processed_data/yTest'\n",
    "    fN_yTr = '/home/user/Documents/git/QuickML/pre_processed_data/yTrain'\n",
    "\n",
    "    # pd.to_csv creates the file if it does not exist, but it does not \n",
    "    # create any non existent directories. The pre_processed_data directory \n",
    "    # already exists, pd.to_csv <i>creates</i> the files and populates them \n",
    "    # with the contents of their respective components. \n",
    "    xTest.to_csv(fN_xT)\n",
    "    xTrain.to_csv(fN_xTr)\n",
    "    yTest.to_csv(fN_yT)\n",
    "    yTrain.to_csv(fN_yTr)\n",
    "\n",
    "    # Getting the file out of the whole path and converting it to a dataframe.\n",
    "    dF = pd.read_csv(file.split('/')[-1])\n",
    "    \n",
    "    # Columns still hard coded! Fix before deploying to production. \n",
    "    col = dF.columns\n",
    "\n",
    "    # return formattes string which contains HTML and HTML tables using \n",
    "    # the 'tabulate' module\n",
    "    return (f'''\n",
    "            <h2 style=\"text-align:center\">Scroll to Preview your Pre-Processed Data!</h2>\n",
    "            <hr>\n",
    "            <div>\n",
    "                <h3 style=\"text-align:left\"> X train </h3> \n",
    "                <h3 style=\"text-align:right; margin-top:-40px\"> Y train </h3> <hr><br>\n",
    "                <div class=\"container\" style=\"display:flex; width=70%\">\n",
    "                    {tabulate(table['X_train'], tablefmt='html', headers = col)}\n",
    "                    {tabulate(table['y_train'], tablefmt='html', headers = col[4:])}\n",
    "                </div>\n",
    "                <hr>\n",
    "                <h3 style=\"text-align:left\"> X test </h3> \n",
    "                <h3 style=\"text-align:right; margin-top:-40px\"> Y test </h3> <hr><br>\n",
    "                <div class=\"container\" style=\"display:flex; width=70%\">\n",
    "                    {tabulate(table['X_test'], tablefmt='html', headers = col)}\n",
    "                    {tabulate(table['y_test'], tablefmt='html', headers = col[4:])}\n",
    "                </div>\n",
    "            </div>\n",
    "    ''' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also writed 4 csv files each containing one of the components of the pre-processed data:\n",
    "1. X_train\n",
    "2. y_train \n",
    "\n",
    "These are datasets which will be used to train the Machine Learning/Deep Learning model. \n",
    "\n",
    "3. X_test\n",
    "4. y_train\n",
    "\n",
    "These are the datasets which are given to the ML/DL model to test it's accruacy. Based on these results, the confusion matrix is created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to keep a reference to the users choice of algorithm so that the correct one is invoked. This is sent from the JavaScript to the Flask backend, which then writes it to a text file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// JavaScript Code \n",
    "document.querySelectorAll('.choice').forEach(item => {\n",
    "    item.addEventListener(\"click\", event => {\n",
    "      var value = item.value;\n",
    "        \n",
    "      // Users choice \n",
    "      let option = value\n",
    "      console.log(option)\n",
    "      // Open and send information to Flask\n",
    "      const request = new XMLHttpRequest()\n",
    "      request.open('POST', `/ProcessOption/${JSON.stringify(option)}`)\n",
    "      request.send();\n",
    "    })\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask Backend Code \n",
    "@views.route('/ProcessOption/<string:option>', methods=['POST'])\n",
    "def SaveOption(option):\n",
    "    sel = json.loads(option)\n",
    "\n",
    "    with open(\"choice.txt\", \"w\") as fo:\n",
    "        fo.write(sel)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This marks the end of the data pre-processing section. Now the algorithms can be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Simple Linear Regression is a machine learning model used on data sets with 2 columns, one independent and one dependent variable. Below is the code for the algorithm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE LINEAR REGRESSION\n",
    "def simpleLinearRegression(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    \"\"\"\n",
    "    Takes the train and test split of the dataset, as well as name\n",
    "    of the uploaded dataSet. Fits a regressor and plots a simple\n",
    "    linear regression on the dataset. Saves the figure and returns path\n",
    "    to saved figure as jpg.\n",
    "    \"\"\"\n",
    "    \n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(Xtrain, Ytrain)   \n",
    "\n",
    "    plt.title(f'Linear Regression for Dataset: {dataSet}')\n",
    "\n",
    "    plt.scatter(Xtest[:,:].transpose()[1:,:].tolist()[0], \n",
    "                Ytest[:,:].transpose()[1:,:].tolist()[0], \n",
    "                color='blue',   \n",
    "                label='Test Samples')\n",
    "    \n",
    "    plt.scatter(Xtrain[:,:].transpose()[1:,:].tolist()[0],\n",
    "                Ytrain[:,:].transpose()[1:,:].tolist()[0],\n",
    "                color='red', \n",
    "                label = 'Train Samples')\n",
    "  \n",
    "    XTest_Plot = Xtest[:,:].transpose()[1:,:].tolist()[0]\n",
    "    (Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    Ytrain_temp = regressor.predict(Xtest) \n",
    "\n",
    "    YTrain_Hat_Plot = Ytrain_temp.transpose()[1:,:].tolist()[0]\n",
    "\n",
    "    plt.plot(sorted(XTest_Plot),\n",
    "             sorted(YTrain_Hat_Plot),\n",
    "             label='Regression line')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'/home/user/Documents/git/QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'/home/user/Documents/git/QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x \n",
    "\n",
    "    # The function returns the path to the figure, which can then be viewed\n",
    "    # by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate linear Regression is the same as a simple linear regression except it accepts multiple independent variables as well as one independent variable. Below is the code for the Multivariate Linear Regression API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIPLE LINEAR REGRESSION \n",
    "def multipleLinearRegression(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. It saves the graph as a figure and returns it \n",
    "    to be later displayed in the html.  \n",
    "    '''\n",
    "\n",
    "    # Fitting Multiple Linear Regression to Training Set \n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(Xtrain, Ytrain)\n",
    "\n",
    "    # Test set prediction \n",
    "    Ypred = regressor.predict(Xtest)\n",
    "    Ypred2 = regressor.predict(Xtrain)\n",
    "\n",
    "    plt.title(f'Multivariate Linear Regression for Dataset: {dataSet}')\n",
    "\n",
    "     # Adding train and test plot \n",
    "    train_plot = plt.subplot(121)\n",
    "    test_plot = plt.subplot(122)\n",
    "    # Setting size of figure\n",
    "    mpl.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "    # Formatting the predictions for plotting\n",
    "    YTest_Hat_Plot = Ypred.transpose()[1:,:].tolist()[0]\n",
    "    YTrain_Hat_Plot = Ypred2.transpose()[1:,:].tolist()[0]\n",
    "\n",
    "    # Train set Plotted \n",
    "    train_plot.grid(True)\n",
    "    train_plot.set_title('Train Set')\n",
    "\n",
    "    # Scattering Actual Train Set\n",
    "    train_plot.scatter(Xtrain[:,:].transpose()[1:,:].tolist()[0], \n",
    "                       Ytrain[:,:].transpose()[1:,:].tolist()[0],\n",
    "                       color ='Orange',\n",
    "                       label = 'Actual Train Set')    \n",
    "\n",
    "    # Scattering Predicted Train Set \n",
    "    train_plot.scatter(Xtrain[:,:].transpose()[1:,:].tolist()[0], \n",
    "                       YTrain_Hat_Plot,\n",
    "                       color ='Green',\n",
    "                       label = 'Predicted Train Set')\n",
    "    train_plot.legend()    \n",
    "\n",
    "    # Test set  Plotted\n",
    "    test_plot.grid(True)\n",
    "    test_plot.set_title('Test Set')\n",
    "\n",
    "    test_plot.scatter(Xtest[:,:].transpose()[1:,:].tolist()[0], \n",
    "                      Ytest[:,:].transpose()[1:,:].tolist()[0],\n",
    "                      color ='Red',\n",
    "                      label = 'Actual Test Set')\n",
    "    test_plot.scatter(Xtest[:,:].transpose()[1:,:].tolist()[0], \n",
    "                      YTest_Hat_Plot,\n",
    "                      color = 'Blue',\n",
    "                      label = 'Predicted Test Set') \n",
    "    test_plot.legend()          \n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test and train sets are both scattered on the graph, and their respective predictions are scattered as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Polynomial Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Linear Regression is similar to Multivariate Linear Regression but should be u sed when the dataset appears to contain non-linear associations. It is still called a 'linear' regression becase the coefficients are linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A polynomial linear regression ignores certain parameters, therefore the variable mapping had to be changed to take into account the ignored variables. The user selects which attributes are to be ignored, and these are then dynamically dropped from their dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for the polynomial linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLYNOMIAL LINEAR REGRESSION \n",
    "def polynomialLinearRegression(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    \n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = Xtrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    XTest = Xtest[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTrain = Ytrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTest = Ytest[:,:].transpose()[1:,:].tolist()[0]\n",
    "\n",
    "    # PLR doesn't need a train test split, so the individual components \n",
    "    # are combined to form the original dataset (except now its pre-processed)\n",
    "    X_combined = np.r_[XTrain, XTest]\n",
    "    Y_combined = np.r_[YTrain, YTest]\n",
    "\n",
    "    \n",
    "    X_combined = np.array(X_combined, dtype='int')\n",
    "    Y_combined = np.array(Y_combined, dtype='int')\n",
    "\n",
    "    # Might not be necessary... delete if deemed unworthy. \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_combined.reshape(-1,1), Y_combined)\n",
    "\n",
    "    # Creating a polynomial regressor\n",
    "    poly_reg = PolynomialFeatures(degree=3)\n",
    "    # Transforming X from just X to X + its polynomial terms\n",
    "    X_Comb_Poly = X_combined.reshape(-1,1)\n",
    "    X_poly = poly_reg.fit_transform(X_Comb_Poly)\n",
    "\n",
    "    # New linear regression fitted onaugmented X matrix and \n",
    "    # original Y vector. \n",
    "    lin_reg2 = LinearRegression()\n",
    "    lin_reg2.fit(X_poly, Y_combined)\n",
    "\n",
    "    plt.title(f'Polynomial Linear Regression for {dataSet}')\n",
    "\n",
    "    # Scattering actual results\n",
    "    plt.scatter(X_combined, Y_combined, color = 'red', label ='Actual')\n",
    "\n",
    "    # Plotting predicted values via linear regression\n",
    "    plt.plot(X_combined, \n",
    "             lin_reg.predict(X_combined.reshape(-1,1)), \n",
    "             color = 'blue', \n",
    "             label = 'Linear')\n",
    "\n",
    "    print(f'{X_poly}============')\n",
    "\n",
    "    X_combined_Plot = X_combined.tolist()\n",
    "    X_poly_Plot = X_poly.tolist()\n",
    "\n",
    "    # Plotting predicted values via polynomial regression\n",
    "    plt.plot(sorted(X_combined_Plot),\n",
    "             lin_reg2.predict(sorted(X_poly_Plot)), \n",
    "             # lin_reg2.predict(poly_reg.fit_transform(X_combined.reshape(-1,1))), \n",
    "             color = 'green', \n",
    "             label = 'Poylnomial')\n",
    "\n",
    "\n",
    "    plt.legend()          \n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the train and test sets have been re-merged so as to properly perform the polynomial linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Polynomial Linear Regression. A kernel is chosen and the algorithm is invoked accordingly. Below is the code for the SVR API: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUPPORT VECTOR REGRESSION \n",
    "def supportVectorRegression(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    \n",
    "    mpl.rcParams['figure.figsize'] = [11, 6]\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = Xtrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    XTest = Xtest[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTrain = Ytrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTest = Ytest[:,:].transpose()[1:,:].tolist()[0]\n",
    "\n",
    "    # SVR doesn't need a train test split, so the individual components \n",
    "    # are combined to form the original dataset (except now its pre-processed)\n",
    "    X_combined = np.r_[XTrain, XTest]\n",
    "    Y_combined = np.r_[YTrain, YTest]\n",
    "\n",
    "    #==========================================================#\n",
    "\n",
    "    sc_X = StandardScaler()\n",
    "    sc_Y = StandardScaler()\n",
    "    X_combined = sc_X.fit_transform(X_combined.reshape(-1,1))\n",
    "    Y_combined = sc_Y.fit_transform(Y_combined.reshape(-1,1))\n",
    "\n",
    "    # Creating an SVR regressor\n",
    "    svr_reg = SVR(kernel='poly')\n",
    "    svr_reg.fit(X_combined, Y_combined)\n",
    "\n",
    "\n",
    "    plt.title(f'Support Vector Regression for {dataSet}')\n",
    "\n",
    "    # Scattering actual results\n",
    "    plt.scatter(X_combined, Y_combined, color = 'blue', label ='Actual')\n",
    "\n",
    "    X_combined_Plot = X_combined.tolist()\n",
    "\n",
    "    # Plotting predicted values via linear regression\n",
    "    plt.plot(sorted(X_combined_Plot), \n",
    "             sorted(svr_reg.predict(X_combined)), \n",
    "             color = 'orange', \n",
    "             label = 'Support Vector')\n",
    "\n",
    "\n",
    "    plt.legend()          \n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K Nearest Neighbours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification algorithm which returns a confusion matrix to illustrate the accuracy and loss ofthe algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "def K_Nearest_Neighbours(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Creating KNN Classifier & Fitting on to Train Set \n",
    "    classifier_K = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p = 2)\n",
    "    classifier_K.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    # Preditcing the Test set Results\n",
    "    YPred = classifier_K.predict(XTest.reshape(-1,1))\n",
    "\n",
    "    # Making the Confusion Matrix\n",
    "    cm = confusion_matrix(YTest, YPred)\n",
    " \n",
    "\n",
    "    clf = SVC(random_state = 0)\n",
    "    clf.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    plot_confusion_matrix(clf, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'K Nearest Neighbours Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "\n",
    "    # plt.matshow(dcm)\n",
    "\n",
    "\n",
    "            \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two classes of points, SVM will find a line to separate the two of them and then classify any new data points depending on which side of the line they fall on. The way it picks the line is it uses the max-margin method. The line whih has the maximum equidistant margin from two points will be chosen as the final line. The two points from which it is equidistant act as the support points (or support vectors) and hence the name of this ML model: Support Vector Machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "def Support_Vector_Machine(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. Performs Support Vector Machine classification\n",
    "    on the given dataset and returns a confusion matrix to display the accuracy\n",
    "    and loss of the ML model. \n",
    "    '''\n",
    "    \n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Create SVM Classifier \n",
    "    classifier = SVC(kernel = 'linear', random_state=0)\n",
    "\n",
    "    # Fit Classifier on to Data \n",
    "    classifier.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    YPred = classifier.predict(XTest.reshape(-1,1))\n",
    "\n",
    "    # Making the Confusion Matrix\n",
    "    cm = confusion_matrix(YTest, YPred)\n",
    " \n",
    "\n",
    "    clf = SVC(random_state = 0)\n",
    "    clf.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    # Plotting the Confusion Matrix \n",
    "    plot_confusion_matrix(clf, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'Support Vector Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "    \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is used when SVM won't do. SVM only works when the data is already linearly seperable, when that is not the case, K-SVM should be used. Using a gaussian kernel, K-SVM maps the input dataset to an additional dimension so that the data inputted becomes linearly seperable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel-Support Vector Machine (SVM)\n",
    "def Kernel_Support_Vector_Machine(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. Performs Kernel-Support Vector Machine \n",
    "    classification on the given dataset and returns a confusion matrix \n",
    "    to display the accuracy and loss of the ML model. \n",
    "    '''\n",
    "    \n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Creating K-SVM Classifier with Gaussian Kernel\n",
    "    classifier = SVC(kernel='rbf', random_state=0)\n",
    "\n",
    "     # Fit Classifier on to Data \n",
    "    classifier.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    YPred = classifier.predict(XTest.reshape(-1,1))\n",
    "\n",
    "    # Making the Confusion Matrix\n",
    "    # cm = confusion_matrix(YTest, YPred)\n",
    " \n",
    "    print('here=================================================')\n",
    "\n",
    "    clf = SVC(random_state = 0)\n",
    "    clf.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    # Plotting the Confusion Matrix \n",
    "    plot_confusion_matrix(clf, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'Kernel-Support Vector Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "    \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a probabilistic algorithm thatâ€™s typically used for classification problems. Naive Bayes is simple, intuitive, and yet performs surprisingly well in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "def Naive_Bayes(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. Performs Naive Bayes classification\n",
    "    on the given dataset and returns a confusion matrix to display the accuracy\n",
    "    and loss of the ML model. The classification is based on Bayes theoreum.\n",
    "    '''\n",
    "    \n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Creating Naive Bayes Classifier\n",
    "    classifier = GaussianNB()\n",
    "\n",
    "    # Fit classifier to training set\n",
    "    classifier.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    YPred = classifier.predict(XTest.reshape(-1,1))\n",
    "\n",
    "    # Making the Confusion Matrix\n",
    "    cm = confusion_matrix(YTest, YPred)\n",
    " \n",
    "    # Plotting the Confusion Matrix \n",
    "    plot_confusion_matrix(classifier, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'Naive Bayes Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "    \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision Tree Classification\n",
    "def Decision_Tree_Classfication(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. Performs Decision Tree classification\n",
    "    on the given dataset and returns a confusion matrix to display the accuracy\n",
    "    and loss of the ML model. \n",
    "    '''\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Creating a Decision Tree Classifer with a 0 random state\n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "     # Fit Classifier on to Data \n",
    "    classifier.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    # Plotting the Confusion Matrix \n",
    "    plot_confusion_matrix(classifier, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'Decision Tree Vector Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "    \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forst Classification\n",
    "def Random_Forest_Classfication(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "    '''\n",
    "    Takes pre processed data and the dataSet which expects the algorithm\n",
    "    to be placed on its data. Performs Random Forest classification\n",
    "    on the given dataset and returns a confusion matrix to display the accuracy\n",
    "    and loss of the ML model. \n",
    "    '''\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = np.array(Xtrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    XTest = np.array(Xtest[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTrain = np.array(Ytrain[:,:].transpose()[1:,:].tolist()[0])\n",
    "    YTest = np.array(Ytest[:,:].transpose()[1:,:].tolist()[0])\n",
    "\n",
    "    # Manually casting to int using labelEncoder class to preserve\n",
    "    # data integrity. (better than .astype('int'))\n",
    "\n",
    "    lab_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "    XTrain = lab_enc.fit_transform(XTrain)\n",
    "    XTest = lab_enc.fit_transform(XTest)\n",
    "    YTrain = lab_enc.fit_transform(YTrain)\n",
    "    YTest = lab_enc.fit_transform(YTest)\n",
    "\n",
    "    # Creating a Decision Tree Classifer with a 0 random state\n",
    "    classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "     # Fit Classifier on to Data \n",
    "    classifier.fit(XTrain.reshape(-1,1), YTrain.reshape(-1,1))\n",
    "\n",
    "    # Plotting the Confusion Matrix \n",
    "    plot_confusion_matrix(classifier, XTest.reshape(-1,1), YTest.reshape(-1,1))\n",
    "\n",
    "    plt.title(f'Random Forest Classification for {dataSet}')\n",
    "\n",
    "    plt.legend() \n",
    "    \n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 K-Means Clutering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Clustering\n",
    "def kMeansClustering(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = Xtrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    XTest = Xtest[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTrain = Ytrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTest = Ytest[:,:].transpose()[1:,:].tolist()[0]\n",
    "\n",
    "\n",
    "    X_combined = np.r_[XTrain, XTest]\n",
    "    Y_combined = np.r_[YTrain, YTest]\n",
    "\n",
    "    # Manually casting to int \n",
    "    X_combined = np.array(X_combined, dtype='int')\n",
    "    Y_combined = np.array(Y_combined, dtype='int')\n",
    "\n",
    "    # Within Cluster sum of Squares\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kMeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kMeans.fit(X_combined.reshape(-1,1))\n",
    "        wcss.append(kMeans.inertia_)\n",
    "\n",
    "    plt.plot(range(1,11), wcss)\n",
    "    plt.title(f'K-Means Clustering for {dataSet}')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical Clustering\n",
    "def hierarchicalClustering(Xtest, Xtrain, Ytest, Ytrain, dataSet):\n",
    "\n",
    "    # Formatting the dataSets for analysis\n",
    "    XTrain = Xtrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    XTest = Xtest[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTrain = Ytrain[:,:].transpose()[1:,:].tolist()[0]\n",
    "    YTest = Ytest[:,:].transpose()[1:,:].tolist()[0]\n",
    "\n",
    "\n",
    "    X_combined = np.r_[XTrain, XTest]\n",
    "    Y_combined = np.r_[YTrain, YTest]\n",
    "\n",
    "    # Manually casting to int \n",
    "    X_combined = np.array(X_combined, dtype='int')\n",
    "    Y_combined = np.array(Y_combined, dtype='int')\n",
    "\n",
    "    #Using a Dendrogram to find the optimal number of clusters\n",
    "    dendrogram = sch.dendrogram(sch.linkage(X_combined.reshape(-1,1), method='ward'))\n",
    "    # The ward method aims to minimize the variance among the clusters\n",
    "   \n",
    "    plt.title(f'Hierarchical Clustering for {dataSet}')\n",
    "    # plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Euclidean Distances')\n",
    "\n",
    "    filename = f'{random.randint(100,999)}'\n",
    "    plt.savefig(f'../QuickML/webapp/static/{filename}.jpg')\n",
    "\n",
    "    x = f'../QuickML/webapp/static/{filename}.jpg'\n",
    "\n",
    "    return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd0a3c9d82cfdbe54ad0cdadc8b7be97191499d59b52a871d890c9444dc8d1c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
